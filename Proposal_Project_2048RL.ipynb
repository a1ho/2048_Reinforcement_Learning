{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 188 - Project Proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Names\n",
    "\n",
    "- Ashley Ho\n",
    "- Mizuho Fukuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract \n",
    "This section should be short and clearly stated. It should be a single paragraph <200 words.  It should summarize: \n",
    "- what your goal/problem is\n",
    "- what the data used represents and how they are measured\n",
    "- what you will be doing with the data\n",
    "- how performance/success will be measured"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "2048 is a single-player tile-sliding game that is played on a 4x4 grid. The game begins with two tiles labeled either $2$ (with probability $0.9$) or $4$ (with probability $0.1$) at random locations on the grid. At each state, the player can move in one of four directions: up, down, left or right, with each move shifting all current tiles on the board in the specified direction within the bounds of the grid. When two tiles of the same value collide during a move, they merge into a single tile labeled with the sum of the two values. Also, on every move, either a new $2$ tile or $4$ tile appears at a random open cell on the grid with probabilities $0.9$ and $0.1$, respectively. If the grid is full with no more allowable moves, the game is over; note that in situations where the grid is full but one of the four moves allows at least one set of two tiles to merge into one, the game is not over yet. In addition, if the objective $2048$ tile is achieved, the game continues on and the player can attempt to compile tiles even higher than $2048$ until the grid is full with no possible moves available. \n",
    "\n",
    "The stochastic nature of 2048 makes it a strong problem to explore using reinforcement learning. In particular, there have been several studies conducted on the effectiveness of reinforcement learning in achieving the desired $2048$ tile by modeling the problem as a Markov Decision Process. One study explored using both deep Q-learning and the beam search algorithm to solve the game, finding that the beam search algorithm, which implemented a heuristic function modeled after human strategy, performed better than the deep Q-learning<a name=\"Li\"></a>[<sup>[1]</sup>](#LiNote). Specifically, the beam algorithm utilized in this paper implemented a heuristic function that modeled a typical human player's strategy of keeping higher-valued tiles towards the corners of the grid, making it easier to merge tiles. Another study utilizes optimistic temporal difference learning, which was able to achieve a $32768$ tile $72\\%$ of the time, which well exceeds the objective $2048$ tile<a name=\"Guei\"></a>[<sup>[2]</sup>](#GueiNote). This study also explores the techniques of Monte Carlo tree search and deep Q-learning. In our project, we will be attempting to replicate aspects of the models mentioned in these studies, with an emphasis on deep Q-learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "You should have a strong idea of what dataset(s) will be used to accomplish this project. \n",
    "\n",
    "If you know what (some) of the data you will use, please give the following information for each dataset:\n",
    "- link/reference to obtain it\n",
    "- description of the size of the dataset (# of variables, # of observations)\n",
    "- what an observation consists of\n",
    "- what some critical variables are, how they are represented\n",
    "- any special handling, transformations, cleaning, etc will be needed\n",
    "\n",
    "If you don't yet know what your dataset(s) will be, you should describe what you desire in terms of the above bullets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Solution\n",
    "\n",
    "In this section, clearly describe a solution to the problem. The solution should be applicable to the project domain and appropriate for the dataset(s) or input(s) given. Provide enough detail (e.g., algorithmic description and/or theoretical properties) to convince us that your solution is applicable. Why might your solution work? Make sure to describe how the solution will be tested.  \n",
    "\n",
    "If you know details already, describe how (e.g., library used, function calls) you plan to implement the solution in a way that is reproducible.\n",
    "\n",
    "If it is appropriate to the problem statement, describe a benchmark model<a name=\"sota\"></a>[<sup>[3]</sup>](#sotanote) against which your solution will be compared. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ethics & Privacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team Expectations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Timeline Proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Footnotes\n",
    "<!-- 1.^: Lorenz, T. (9 Dec 2021) Birds Aren’t Real, or Are They? Inside a Gen Z Conspiracy Theory. The New York Times. https://www.nytimes.com/2021/12/09/technology/birds-arent-real-gen-z-misinformation.html -->\n",
    "\n",
    "<a name='Li'></a>1.[^](#LiNote): Li, S., and Peng, V. (20 Oct 2021) Playing 2048 with Reinforcement Learning. *arXiv.Org*. https://doi.org/10.48550/arXiv.2110.10374 <br> \n",
    "<a name=”Guei”></a>2.[^](#GueiNote): Guei, H. (21 Dec 2022) On Reinforcement Learning for the Game of 2048. *arXiv.Org*. https://doi.org/10.48550/arXiv.2212.11087 <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
