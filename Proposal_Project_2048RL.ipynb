{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 188 - Project Proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Names\n",
    "\n",
    "- Ashley Ho\n",
    "- Mizuho Fukuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract \n",
    "\n",
    "This project attempts to build a model that solves the 2048 game using reinforcement learning algorithms. 2048 is a single-player tile puzzle game in which the main objective is to produce a large value tile by merging tiles of powers of 2 by sliding the tiles either up, down, left, or right at each state. The model will be trained using both Q-learning and deep Q-learning algorithims on randomly simulated game environments. The reward mechanisms of the above algorithms will attempt to replicate human strategies as a way of building an intelligent agent. The performance of these models will be compared to a baseline model that chooses a random action at each state. The evaluation metrics for the models will be both the average score over 1000 games and the distribution of maximum tile-values achieved. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "2048 is a single-player tile-sliding game that is played on a 4x4 grid. The game begins with two tiles labeled either $2$ (with probability $0.9$) or $4$ (with probability $0.1$) at random locations on the grid. At each state, the player can move in one of four directions: up, down, left or right, with each move shifting all current tiles on the board in the specified direction within the bounds of the grid. When two tiles of the same value collide during a move, they merge into a single tile labeled with the sum of the two values. Also, on every move, either a new $2$ tile or $4$ tile appears at a random open cell on the grid with probabilities $0.9$ and $0.1$, respectively. If the grid is full with no more allowable moves, the game is over; note that in situations where the grid is full but one of the four moves allows at least one set of two tiles to merge into one, the game is not over yet. In addition, if the objective $2048$ tile is achieved, the game continues on and the player can attempt to compile tiles even higher than $2048$ until the grid is full with no possible moves available. \n",
    "\n",
    "The stochastic nature of 2048 makes it a strong problem to explore using reinforcement learning. In particular, there have been several studies conducted on the effectiveness of reinforcement learning in achieving the desired $2048$ tile by modeling the problem as a Markov Decision Process. One study explored using both deep Q-learning and the beam search algorithm to solve the game, finding that the beam search algorithm, which implemented a heuristic function modeled after human strategy, performed better than the deep Q-learning<a name=\"Li\"></a>[<sup>[1]</sup>](#LiNote). Specifically, the beam algorithm utilized in this paper implemented a heuristic function that modeled a typical human player's strategy of keeping higher-valued tiles towards the corners of the grid, making it easier to merge tiles. Another study utilizes optimistic temporal difference learning, which was able to achieve a $32768$ tile $72\\%$ of the time, which well exceeds the objective $2048$ tile<a name=\"Guei\"></a>[<sup>[2]</sup>](#GueiNote). This study also explores the techniques of Monte Carlo tree search and deep Q-learning. In our project, we will be attempting to replicate aspects of the models mentioned in these studies, with an emphasis on Q-learning and deep Q-learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "This project attempts to solve the game 2048 using reinforcement learning algorithms, where we define 'solving' as reaching a maximium tile of $2048$. The objective for the model is to achieve as high of a tile as possible without losing the game (i.e. getting a full grid with no more allowable moves). The game can quantified as a $4x4$ matrix, with a fixed set of actions available at each state. The performance of the model can be measured by the score at the end of the game, which is accumulated at each move by adding the merged tile values. In addition, the setup of this game is replicable due to based on its stochasticity and the clearly defined environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "Since the game environment of 2048 follows a relatively simple and easily-replicable set of rules, we will be generating our own data. More specifically, the data will depend on previous sets of actions on a specific board, and thus the only way to train a model would be to generate random tiles at each state. Currently, we have found several packages available on the Python Package Index and GitHub that generate the game environment, which will help us to visualize the board at each state:\n",
    "1. [gym-2048 0.2.6](https://pypi.org/project/gym-2048/)\n",
    "1. [2048 0.3.3](https://pypi.org/project/2048/)\n",
    "1. [Python2048](https://github.com/plemaster01/Python2048)\n",
    "\n",
    "Although we have not solidified a mathematical formulation for the reward mechanisms to train the model, we have the following outline:\n",
    "- reward for keeping the largest-valued tiles on one side of the grid, an action that will make it easier to merge tiles\n",
    "- reward for increasing the score so that model will be encouraged to make the move that maximizes the score at that state\n",
    "- reward for the number of empty tiles on the board \n",
    "\n",
    "In general, these reward mechanisms should attempt to adopt human-like strategies in order to maximize its chances of solving the puzzle. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Solution\n",
    "\n",
    "We will attempt to train a Q-learning and/or a deep Q-learning model that would solve the 2048 game (we define 'solving' as achieving a max tile of 2048). For Q-learning, we will use the following update formula to update the Q value for each state and action: \n",
    "    $$Q_{t+1}(A_t) = Q_t(A_t) + \\alpha_t (R_t - Q_t(A_t))$$\n",
    "We will be using epsilon-greedy algorithm for action selection. At each state, there is a maximum of 4 possible actions, therefore, the search space is small enough for a greedy algorithm.\n",
    "In addition, we will also be attempting to compare this model performance with a deep Q-learning model. We will be referencing several online examples<a name=\"model\"></a>[<sup>[3]</sup>](#modelNote) building the model architecture. Based on our research, most deep Q-learning architectures include 2 to 3 convolutional layers. We will experiment with different reward mechanisms to train the network.\n",
    "\n",
    "The baseline model for comparison would be a completely random algorithm, in which the action at each state is chosen at random. Both the baseline and trained models will be tested on 1000 simulated games and will be evaluated based on the evaluation metrics discussed below. We expect the trained models to perform significantly better than the baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "The main evaluation metric that we will utilize in this project is the average score of the games $S_{mean}$. The average score $S_{mean}$ for a total of $N$ games with $a_i$ actions each is defined as:\n",
    "    $$S_{mean} = \\frac{1}{N} \\sum_{i=1}^N S_i$$\n",
    "    $$\\text{where }S_i = \\sum_{k=1}^{a_i} \\text{merged tile values}$$\n",
    "In other words, the total score for a single game is calculated by summing the new tile values formed by merging at every action (according to the scoring formula given by the official 2048 game). The evaluation metric for a model is the average of this value over all $N$ simulated games.\n",
    "\n",
    "In addition to this metric, we will also observe the distribution of maximum tile value achieved by the model over $N$ games as a more intuitive measure of the quality of the model. Note that this metric is directly correlated with the score as defined above. However, this would still be a interesting statistic to observe since it is a more direct measure in relation to the objective of the game, which is to get as high of a number as possible. For example, if two players both end the game with a 2048 maximum tile, the results can be considered as equal in practice despite potential differences in total scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Average Score | Std. Dev. of Scores |\n",
    "|:--- |:--- |\n",
    "| Baseline | 1096.04 | 493.48 |\n",
    "| DQN | 1508.48 | 833.89 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ethics & Privacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We anticipate minimal issues regarding ethics and privacy with this project. Since we will not be using any outside data, it will not infringe on any personal privacy. In addition, since 2048 is a single player game, even if the model is used to cheat on the game, it will not affect other players. The official 2048 game itself was created by Italian web developer Gabriele Cirulli and is a free and open-source software. Thus, for the purposes of this project, there is no risk of copyright infringement. Any outside sources that may be used in this project to simulate the game environment as well as for building the model will be clearly indicated and included in the reference section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team Expectations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Divide the work evenly\n",
    "* Communicate regularly via Messages\n",
    "* Be prepared at every meeting\n",
    "* Ask for help if confused"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Timeline Proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Meeting Date  | Meeting Time| Completed Before Meeting  | Discuss at Meeting |\n",
    "|---|---|---|---|\n",
    "| 5/16  |  11 AM |  Brainstorm topics/questions  | Decide topic and draft proposal | \n",
    "| 5/17  |  12 PM |  Finish individual writeup sections | Finalize and submit proposal | \n",
    "| 5/24  | 10 AM  | Read through feedback and think about code  | Figure out 2048 environment   |\n",
    "| 5/28  | 2 PM  | TBD (depends on what we get done in previous meeting) | Solidify game model  |\n",
    "| 5/30  | 12 PM  | N/A | Grid search/feature selection for best policy/reward function etc. |\n",
    "| 6/6  | 1 PM  | Finish individual writeup sections if necessary| Discuss/edit full project |\n",
    "| 6/12  | Before 11:59 PM  | NA | Turn in Final Project  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Footnotes\n",
    "<!-- 1.^: Lorenz, T. (9 Dec 2021) Birds Aren’t Real, or Are They? Inside a Gen Z Conspiracy Theory. The New York Times. https://www.nytimes.com/2021/12/09/technology/birds-arent-real-gen-z-misinformation.html -->\n",
    "\n",
    "<a name='Li'></a>1.[^](#LiNote): Li, S., and Peng, V. (20 Oct 2021) Playing 2048 with Reinforcement Learning. *arXiv.Org*. https://doi.org/10.48550/arXiv.2110.10374 <br> \n",
    "<a name=”Guei”></a>2.[^](#GueiNote): Guei, H. (21 Dec 2022) On Reinforcement Learning for the Game of 2048. *arXiv.Org*. https://doi.org/10.48550/arXiv.2212.11087 <br>\n",
    "<a name=\"model\"></a>3.[^](#modelNote): Virdee, N. (2018) 2048-Deep-Reinforcement-Learning. https://github.com/navjindervirdee/2048-deep-reinforcement-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
