{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 188 - Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2048 Using Reinforcement Learning\n",
    "\n",
    "## Group members\n",
    "\n",
    "- Ashley Ho\n",
    "- Mizuho Fukuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract \n",
    "This project builds a model that attempts to solve the 2048 game using reinforcement learning. 2048 is a single-player tile puzzle game in which the main objective is to produce a large value tile by merging tiles of powers of 2 by sliding the tiles either up, down, left, or right at each state. We compare a baseline model that randomly chooses a move at each state, with a Deep Q-Network (DQN) model, implemented using Stable-Baselines3, that attempts to imitate human strategies. The evaluation metrics include the average score over 100 games, the standard deviation of the scores, and the highest tile achieved for each game over 100 games. The baseline model achieved an average score of 1050.32, frequently reaching the $128$ tile but rarely progressing beyond $256$. In contrast, the DQN model significantly improved performance with an average score of 1972.32 and more frequent attainment of higher tiles, including a $5\\%$ achievement rate of the $512$ tile. These findings highlight the DQN model's capability to navigate the game's dynamics, evidenced by consistently higher scores and more advanced tile achievements. Although the DQN model was ultimately unable to achieve the $2048$ tile, the results highlight the effectiveness of deep reinforcement learning in complex game environments, suggesting potential for further optimization to consistently achieve even higher tiles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "2048 is a single-player tile-sliding game that is played on a 4x4 grid. The game begins with two tiles labeled either $2$ (with probability $0.9$) or $4$ (with probability $0.1$) at random locations on the grid. At each state, the player can move in one of four directions: up, down, left or right, with each move shifting all current tiles on the board in the specified direction within the bounds of the grid. When two tiles of the same value collide during a move, they merge into a single tile labeled with the sum of the two values. Also, on every move, either a new $2$ tile or $4$ tile appears at a random open cell on the grid with probabilities $0.9$ and $0.1$, respectively. If the grid is full with no more allowable moves, the game is over; note that in situations where the grid is full but one of the four moves allows at least one set of two tiles to merge into one, the game is not over yet. In addition, if the objective $2048$ tile is achieved, the game continues on and the player can attempt to compile tiles even higher than $2048$ until the grid is full with no possible moves available. The general strategy of the game involves maneuvering the tiles in such a way as to combine them to create higher-value tiles, ideally working towards the goal of forming a $2048$ tile. A common tactic employed by players is to keep the highest-value tiles in a specific corner and build up from there, which helps in managing the board more effectively and making higher-value merges easier.\n",
    "\n",
    "The stochastic nature of 2048 makes it a strong problem to explore using reinforcement learning. In particular, there have been several studies conducted on the effectiveness of reinforcement learning in achieving the desired $2048$ tile by modeling the problem as a Markov Decision Process. One study explored using both deep Q-learning and the beam search algorithm to solve the game, finding that the beam search algorithm, which implemented a heuristic function modeled after human strategy, performed better than the deep Q-learning<a name=\"Li\"></a>[<sup>[1]</sup>](#LiNote). Specifically, the beam algorithm utilized in this paper implemented a heuristic function that modeled a typical human player's strategy of keeping higher-valued tiles towards the corners of the grid, making it easier to merge tiles. Another study utilizes optimistic temporal difference learning, which was able to achieve a $32768$ tile $72\\%$ of the time, which well exceeds the objective $2048$ tile<a name=\"Guei\"></a>[<sup>[2]</sup>](#GueiNote). This study also explores the techniques of Monte Carlo tree search and deep Q-learning. In our project, we will be attempting to replicate aspects of the models mentioned in these studies, with an emphasis on Q-learning and deep Q-learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "This project attempts to solve the game 2048 using the Deep Q Network algorithm, where we define 'solving' as reaching a maximum tile of $2048$. The objective for the model is to achieve as high of a tile as possible without losing the game (i.e. getting a full grid with no more allowable moves). The game is quantified as a $4x4$ matrix, with a fixed set of actions available at each state. The performance of the model can be measured by the average score over 100 trials after training, where the score of each trial is calculated by accumulating the merged tile values after each move. In addition, the setup of this game is replicable due its stochasticity and the clearly defined environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "Since the game environment of 2048 follows a relatively simple and easily-replicable set of rules, we generated our own data. More specifically, the data will depend on previous sets of actions on a specific board, and thus the only way to train a model would be to generate random tiles at each state. \n",
    "\n",
    "The environment implementation is designed to emulate the 2048 game using the OpenAI Gym <a name=\"gym\"></a>[<sup>[4]</sup>](#gymNote) interface, providing a structured setup for reinforcement learning experiments. The `Game2048Env` class defines the environment with a 4x4 grid observation space and four discrete actions representing possible moves (up, down, left, right). It uses the `Game2048` class to manage game mechanics such as tile movement, merging, and the addition of new tiles. The `step` function processes the chosen action, updating the game state, calculating rewards, and checking for termination conditions. Additionally, the environment includes rendering capabilities for visualizing the game state, allowing for easier observation of the model behavior.\n",
    "\n",
    "The reward after each action is calculated based on various characteristics of the current state as well as improvements from the previous state:\n",
    "- Base Reward = increase in score (total of merged tiles created by the action)\n",
    "- Bonus Factor:\n",
    "    - $+1$ for every value that in the bottom row that is in the top 5 largest values on the board\n",
    "    - $+log_2(\\text{max value on board})$ if the max value of the board is in the left bottom corner\n",
    "    - $+log_2(\\text{max value on board})$ if the current action results in a new max value\n",
    "    - $+2$ if the bottom row is a gradient with the largest value being the left bottom corner (e.g. [32, 16, 8, 4])\n",
    "- Penalties:\n",
    "    - $-(\\text{number of nonzero tiles on the board})$ if the number of filled spaces on the board exceeds 12 (Â¾ of the board)\n",
    "- Absolute penalties:\n",
    "    - $-5$ if the action is move up\n",
    "    - $-5$ if the action is move right and there is at least one empty space on the bottom row of the board\n",
    "- Game over penalty:\n",
    "    - $-1000$ if the model loses the game\n",
    "\n",
    "Thus, the total reward policy is as follows (define $\\text{a = action})$: \n",
    "<p style=\"text-align: center;\">\n",
    "$ \\text{Total Reward} = \n",
    "\\   \\left\\{\n",
    "\\begin{array}{ll}\n",
    "      -5 & \\text{a=Up or (a=Right & bottom row not full)} \\\\\n",
    "      -1000 & \\text{game ends} \\\\\n",
    "      \\text{Base Reward * Bonus Factor} & \\text{otherwise} \\\\\n",
    "\\end{array} \n",
    "\\right. \\\n",
    "$\n",
    "</p>\n",
    "\n",
    "In general, these reward mechanisms attempt to adopt human-like strategies in order to maximize its chances of solving the puzzle. In particular, the rewards imitate the popular strategy of keeping the largest number at a specified corner and keeping subsequent large values in either the same column or row as that value, ideally in a gradient fashion. This strategy also discourages moves that would potentially disrupt this structure due to the randomness of the spawning of new tiles. In the case of this model, since we want to keep the largest tile at the left bottom corner, we avoid moving up and right to avoid spawning new tiles in the bottom row. However, we also want the model to prioritize saving the game once the board becomes too crowded.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Solution\n",
    "To address this problem, we propose using a Deep Q-Network (DQN) model, a reinforcement learning algorithm that combines Q-learning with deep neural networks. The baseline model is a simple heuristic-based model that takes random actions at each state to move tiles on the game board. This baseline model provides a point of reference to gauge the performance improvement achieved by the DQN model. The baseline model was implemented using a class-based approach in Python, utilizing NumPy for board manipulations and random action selections.\n",
    "The DQN model leverages deep learning to approximate the Q-value function, which predicts the expected future rewards for each action given the current state of the game. The DQN algorithm involves the following key steps:\n",
    "\n",
    "1. Experience Replay: The agent stores its experiences (state, action, reward, next state) in a replay buffer. During training, random batches from this buffer are used to update the Q-network, breaking the correlation between consecutive experiences and improving learning stability.\n",
    "1. Target Network: A separate target network, which is a delayed copy of the Q-network, is used to compute the target Q-values. This helps to stabilize training by reducing oscillations.\n",
    "1. Exploration-Exploitation Trade-off: The agent balances exploration (choosing random actions) and exploitation (choosing actions based on learned Q-values) using an epsilon-greedy policy. The exploration rate decreases over time, allowing the agent to progressively focus on exploiting its knowledge.\n",
    "\n",
    "Due to the dimensional complexity of state-action space, where each move can drastically alter the game board's configuration and future potential actions, traditional heuristic approaches may struggle to capture the long-term dependencies and optimal strategies required to excel in the game. The DQN model is suited for this problem because it leverages deep learning to approximate the Q-value function, effectively handling the high-dimensional state space. By using experience replay and a target network, the DQN model stabilizes training and allows the agent to learn sophisticated strategies through exploration and exploitation. \n",
    "\n",
    "The DQN model was implemented using the Stable-Baselines3 library <a name=\"SB3\"></a>[<sup>[5]</sup>](#SB3Note). The `MlpPolicy` was used as the policy network, which consists of multi-layer perceptrons to approximate the Q-values. The hyperparameters of the DQN model that we tune are learning rate, buffer size, batch size, the discount factor (gamma), the target update interval, training frequency, and exploration factors (epsilon)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "The main evaluation metric that we will utilize is the average score of the games $S_{mean}$ over 100 trials. The average score $S_{mean}$ for a total of $100$ games with $a_i$ actions each is defined as:\n",
    "    $$S_{mean} = \\frac{1}{100} \\sum_{i=1}^{100} S_i$$\n",
    "    $$\\text{where }S_i = \\sum_{k=1}^{a_i} \\text{merged tile values}$$\n",
    "In other words, the total score for a single game is calculated by summing the new tile values formed by merging at every action, which is the scoring formula given by the official 2048 game. The evaluation metric for a model is the average of this value over all 100 simulated games. This metric provides an overall indication of the agent's performance in its attempt to accomplish the objective of the game.\n",
    "\n",
    "In addition to this metric, we observe the distribution of maximum tile values achieved by the model over 100 games as a more intuitive measure of the quality of the model. Note that this metric is directly correlated with the score as defined above. However, this is still an interesting statistic to observe since it is a more direct measure in relation to the objective of the game, which is to get as high of a number as possible. For example, if two players both end the game with a $2048$ maximum tile, the results can be considered as equal in practice despite potential differences in total scores. The distribution of maximum tiles is calculated by simply dividing the number of games that achieved a maximum tile of $T$ (where $T$ is any of the values from $2$, $4$, $8$, $16$, etc.) by 100 to get the percentage.\n",
    "\n",
    "The final metric we use is the standard deviation of the scores over the 100 simulated games. The standard deviation of scores measures the variability or dispersion of the scores achieved by the agent. A lower standard deviation indicates more consistent performance, while a higher standard deviation suggests greater variability. The standard deviation of scores assesses the consistency of the agent's performance across multiple episodes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from env_2048 import Game2048Env\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from IPython.display import display, clear_output, Video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Baseline Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "eps 100: score - 688\r",
      "Average Score: 1050.32\n",
      "Std. Dev. of Scores: 587.0719696936654\n",
      "Max Tiles Achieved out of 100 episodes: {16: 0.02, 32: 0.12, 64: 0.37, 128: 0.4, 256: 0.09}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = Game2048Env()\n",
    "scores = []\n",
    "max_tiles = {}\n",
    "for eps in range(100):\n",
    "    terminated = False\n",
    "    truncated = False\n",
    "    observation, _ = env.reset()\n",
    "    while not terminated or truncated:\n",
    "        action = env.action_space.sample()  # Take a random action\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "        moves = {0: 'up', 1: 'down', 2: 'left', 3:'right'}\n",
    "        if eps == 99:\n",
    "            env.render(mode='rgb_array')\n",
    "        if terminated or truncated:\n",
    "            print(f'\\reps {eps + 1}: score - {env.game.score}', end='')\n",
    "    scores.append(env.game.score)\n",
    "    if np.max(env.game.board) in max_tiles:\n",
    "        max_tiles[np.max(env.game.board)] += 1\n",
    "    else:\n",
    "        max_tiles[np.max(env.game.board)] = 1\n",
    "max_tiles = {key : val/100 for (key, val) in max_tiles.items()}\n",
    "max_tiles = dict(sorted(max_tiles.items()))\n",
    "print(f'''\\rAverage Score: {np.mean(scores)}\n",
    "Std. Dev. of Scores: {np.std(scores)}\n",
    "Max Tiles Achieved out of 100 episodes: {max_tiles}\n",
    "''')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DQN Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "eps 100: score - 2316\t\n",
      "Average Score: 1972.32\n",
      "Std. Dev. of Scores: 1136.3662339228495\n",
      "Max Tiles Achieved out of 100 episodes: {32: 0.03, 64: 0.19, 128: 0.4, 256: 0.33, 512: 0.05}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = Game2048Env()\n",
    "check_env(env, warn=True)\n",
    "\n",
    "policy_kwargs = dict(\n",
    "    net_arch=[256, 256, 256, 256],\n",
    "    activation_fn=torch.nn.ReLU\n",
    ")\n",
    "model = DQN(\n",
    "    'MlpPolicy',\n",
    "    env,\n",
    "    policy_kwargs=policy_kwargs,\n",
    "    verbose=1,\n",
    "    learning_rate=0.0005,\n",
    "    buffer_size=100000,\n",
    "    learning_starts=1000,\n",
    "    batch_size=64,\n",
    "    target_update_interval=500,\n",
    "    gamma=0.99,\n",
    "    train_freq=(4, 'step'),\n",
    "    exploration_fraction=0.4,\n",
    "    exploration_initial_eps=1.0,\n",
    "    exploration_final_eps=0.02\n",
    "    )\n",
    "\n",
    "\n",
    "model.learn(total_timesteps=500000, log_interval=100)\n",
    "model.save(\"dqn_2048\")\n",
    "\n",
    "scores = []\n",
    "max_tiles = {}\n",
    "\n",
    "clear_output(wait = True)\n",
    "\n",
    "for eps in range(100):\n",
    "    terminated = False\n",
    "    truncated = False\n",
    "    obs, _ = env.reset()\n",
    "    while not terminated or truncated:\n",
    "        action, _states = model.predict(obs)\n",
    "        obs, rewards, terminated, truncated, info = env.step(action)\n",
    "        if eps == 99:\n",
    "            env.render('rgb_array')\n",
    "        if terminated or truncated:\n",
    "            print(f'\\reps {eps + 1}: score - {env.game.score}\\t', end='')\n",
    "    scores.append(env.game.score)\n",
    "    if np.max(env.game.board) in max_tiles:\n",
    "        max_tiles[np.max(env.game.board)] += 1\n",
    "    else:\n",
    "        max_tiles[np.max(env.game.board)] = 1\n",
    "\n",
    "max_tiles = {key : val/100 for (key, val) in max_tiles.items()}\n",
    "max_tiles = dict(sorted(max_tiles.items()))\n",
    "print(f'''\n",
    "Average Score: {np.mean(scores)}\n",
    "Std. Dev. of Scores: {np.std(scores)}\n",
    "Max Tiles Achieved out of 100 episodes: {max_tiles}\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Performance measures over 100 episodes**\n",
    "\n",
    "| Model | Average Score | Std. Dev. of Scores |\n",
    "| --- | --- | --- |\n",
    "| Baseline | 1050.32 | 587.07 |\n",
    "| DQN | 1972.32 | 1136.37 |\n",
    "\n",
    "| Max Tile Achieved | Baseline | DQN |\n",
    "| --- | --- | --- |\n",
    "| 16 | 2% | 0% |\n",
    "| 32 | 12% | 3% |\n",
    "| 64 | 37% | 19% |\n",
    "| 128 | 40% | 40% |\n",
    "| 256 | 9% | 33% |\n",
    "| 512 | 0% | 5% |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this study, we evaluated the performance of a baseline model and a Deep Q-Network (DQN) model on the 2048 game. The baseline model achieved an average score of 1050.32 with a standard deviation of 587.07, indicating moderate variability in its performance. The highest tiles achieved by the baseline model across 100 episodes are shown in the above table. The baseline model was reasonably consistent in reaching the 128 tile but rarely progressed beyond the 256 tile. In contrast, the DQN model demonstrated a notable improvement in performance. The average score increased to 1972.32, with a higher standard deviation of 1136.37, reflecting a wider range of scores. The DQN model increased the frequency of achieving higher tiles, specifically 256 and 512. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Hyperparameters**\n",
    "\n",
    "- In this implementation, the `learning_rate` was set to 0.0005. This relatively low learning rate ensures that the updates to the network's weights are small and gradual, which helps in achieving stable learning and avoiding large oscillations in the training process. \n",
    "- The `buffer_size`, set to 100,000 in this model, determines the maximum number of experiences (state, action, reward, next state) that the replay buffer can hold. A larger buffer size allows the model to learn from a more extensive history of past experiences, which can improve the quality of the training data and lead to better generalization. However, a larger buffer also requires more memory and computational resources. The chosen value strikes a balance between having enough experiences to learn from and managing computational efficiency.\n",
    "- The `learning_starts` is set to 1,000 to ensure that the replay buffer has enough experiences to sample from, providing a diverse set of transitions for training the model. This helps in avoiding overfitting to the initial few experiences and allows the agent to explore the environment sufficiently before starting to learn. \n",
    "- The `batch_size` is set to 64. A batch size of 64 is a commonly used value that provides a good trade-off between the stability of the updates and the computational efficiency. \n",
    "- The `target_update_interval` is set to 500. Using a target network helps in stabilizing the training by keeping a fixed set of parameters for calculating the target Q-values over multiple updates. The chosen interval of 500 steps ensures that the target network is updated frequently enough to reflect the current policy while maintaining some stability in the training process.\n",
    "- The discount factor, denoted as `gamma` is set to 0.99. A value of 0.99 indicates that future rewards are slightly less important than immediate rewards but still significantly contribute to the overall expected return. This encourages the agent to consider long-term benefits while making decisions, which is essential for achieving high scores in the 2048 game.\n",
    "- The `train_freq` parameter is set to (4, 'step'), meaning that the model is trained after every 4 steps taken in the environment. This frequency ensures that the model updates its parameters regularly, allowing for continuous learning and adaptation to the environment.\n",
    "- The `exploration_fraction` is set to 0.4, indicating that 40% of the training process will involve exploration. \n",
    "- The `exploration_initial_eps` is set to 1.0, meaning that the agent starts with complete exploration by selecting actions randomly.\n",
    "- This value gradually decreases to the `exploration_final_eps` of 0.02, encouraging the agent to exploit its learned policy while still occasionally exploring new actions to avoid local optima.\n",
    "\n",
    "The policy_kwargs dictionary specifies the architecture and activation function for the neural network used in the DQN model. In this setup, `net_arch=[256, 256, 256, 256]` defines a neural network with four hidden layers, each containing 256 neurons. This deep architecture allows the network to learn complex representations of the game state, which can improve the model's ability to make accurate predictions about the best actions to take. The `activation_fn=torch.nn.ReLU` specifies that the Rectified Linear Unit (ReLU) activation function is used between the layers. ReLU helps mitigate the vanishing gradient problem, allowing the network to train more effectively and converge faster.\n",
    "\n",
    "Since there are a large number of hyperparameters and many of them depend on each other, we arrived at the above set of hyperparameters by hand-picking the numbers through several trials and errors. Ideally, we would perform a grid-search on each of these parameters to arrive at the highest performing mode. However, since there was limited time and our focus was to tune the reward function, we decided to simply hand-pick the hyperparameter values that seemed suitable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualization**\n",
    "\n",
    "The following videos are examples of gameplays using the baseline model and the DQN model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"baseline_2048.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Video(\"baseline_2048.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DQN Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"dqn_2048.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Video(\"dqn_2048.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "### Interpreting the result\n",
    "\n",
    "Observing the rendered animations of the performance of each model, we noticed that although the DQN model does not outperform the baseline model drastically in terms of average score, the efficiency of the two models differ significantly. The tiles colliding with each other in a random fashion can occasionally result in high numbers but this does not happen efficiently and the board tends to be filled with random 2 or 4 tiles that are left unmerged. The DQN model, on the other hand, displays a more efficient gameplay. One can clearly observe the aforementioned strategy of keeping larger numbers at the bottom, which results in the model achieving larger tiles with fewer number of moves.\n",
    "\n",
    "The comparison between the baseline and DQN models highlights the effectiveness of deep reinforcement learning in solving complex tasks like the 2048 game. The baseline model, likely employing heuristic or simpler strategies, reached its performance limits at lower tiles. In contrast, the DQN model, which leverages deep learning to make more sophisticated decisions, was able to achieve higher scores and more advanced tiles more frequently. The increased standard deviation in the DQN model's scores suggests that while it has a higher potential for better performance, it also experiences greater variability, possibly due to the exploratory nature of the reinforcement learning process.\n",
    "\n",
    "Overall, these results demonstrate that the DQN model significantly outperforms the baseline model, making it a more effective approach for playing the 2048 game. The ability of the DQN model to achieve higher tiles and scores indicates its capacity to learn and adapt to the game's dynamics better than the baseline model. This suggests that further improvements and fine-tuning of the DQN model could lead to even better performance, potentially achieving the 1024 or 2048 tiles more consistently.\n",
    "\n",
    "\n",
    "### Limitations\n",
    "\n",
    "While the implementation of a Deep Q-Network (DQN) model for the 2048 game demonstrates notable improvements over the baseline model, there are several limitations to this work.\n",
    "\n",
    "The 2048 game has a highly complex state-action space, where each move can drastically alter the game board's configuration. Even with the advanced capabilities of the DQN model, the vast number of possible states and actions poses significant challenges. The DQN struggles to fully capture and optimize long-term strategies required to consistently achieve the highest tiles, such as 1024 or 2048. This complexity can lead to suboptimal decision-making and variability in performance.\n",
    "\n",
    "In addition, the performance of the DQN model is highly sensitive to its hyperparameters, such as the learning rate, exploration-exploitation parameters, and network architecture. Due to time and computational constraints, the exploration of hyperparameter space was limited. A more exhaustive search or the use of hyperparameter optimization techniques, such as grid search or Bayesian optimization, could potentially yield better-performing models.\n",
    "\n",
    "Finally, while the DQN model shows promise, more advanced reinforcement learning algorithms, such as Double DQN, Dueling DQN, or Actor-Critic methods, and using a custom defined convolutional neural network (CNN) as the DQNâs policy, could be explored to further enhance performance. These models address some of the limitations inherent in standard DQNs and may offer improved learning efficiency and stability.\n",
    "\n",
    "\n",
    "### Ethics & Privacy\n",
    "\n",
    "We anticipate minimal issues regarding ethics and privacy with this project. Since we did not use any outside data, it will not infringe on any personal privacy. In addition, since 2048 is a single player game, even if the model is used to cheat on the game, it will not affect other players. The official 2048 game itself was created by Italian web developer Gabriele Cirulli and is a free and open-source software. Thus, for the purposes of this project, there is no risk of copyright infringement. Any outside sources used in this project to simulate the game environment as well as for building the model will be clearly indicated and included in the reference section.\n",
    "\n",
    "However, it is essential to recognize that most machine learning (ML) projects have potential ethical implications, even if they are not immediately apparent. While this project focuses on a game, the techniques and methodologies developed could be applied to other domains with more significant ethical implications. In addition, although our current project does not use personal data, it's crucial to ensure that the algorithms we develop do not perpetuate biases. In other applications, reinforcement learning algorithms must be carefully monitored to prevent biased decision-making that could harm individuals or groups.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "This study demonstrates the significant advantages of using Deep Q-Networks (DQN) over baseline heuristic models for playing the 2048 game. Our results clearly show that the DQN model achieves higher scores and reaches more advanced tiles, indicating its ability to learn and adapt to the game's dynamics based on specified reward policies. This highlights the effectiveness of deep reinforcement learning in solving complex, strategic tasks, aligning with similar findings in the field of game AI research.\n",
    "\n",
    "In the context of existing work, our project adds to the growing body of evidence that deep learning techniques can outperform traditional methods in various game-playing scenarios. The variability in the DQN modelâs performance suggests that further fine-tuning and exploration of hyperparameters could yield even better results.\n",
    "\n",
    "Future work could involve experimenting with more advanced reinforcement learning algorithms, such as Double DQN or Actor-Critic methods, to enhance stability and performance. Additionally, applying transfer learning techniques might help leverage knowledge from other similar games to improve performance in 2048. By continuing to refine these models, we can contribute to broader applications of AI in strategic decision-making and complex problem-solving.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Footnotes\n",
    "<a name='Li'></a>1.[^](#LiNote): Li, S., and Peng, V. (20 Oct 2021) Playing 2048 with Reinforcement Learning. *arXiv.Org*. https://doi.org/10.48550/arXiv.2110.10374 <br> \n",
    "\n",
    "<a name=âGueiâ></a>2.[^](#GueiNote): Guei, H. (21 Dec 2022) On Reinforcement Learning for the Game of 2048. *arXiv.Org*. https://doi.org/10.48550/arXiv.2212.11087 <br>\n",
    "\n",
    "<a name=\"model\"></a>3.[^](#modelNote): Virdee, N. (2018) 2048-Deep-Reinforcement-Learning. https://github.com/navjindervirdee/2048-deep-reinforcement-learning<br>\n",
    "\n",
    "<a name='gym'></a>4.[^](#gymNote): Gymnasium Documentation. Retrieved June 12, 2024, from https://gymnasium.farama.org/<br> \n",
    "\n",
    "<a name=\"SB3\"></a>5.[^](#SB3Note): Stable-Baselines3 Documentation. Retrieved June 12, 2024, from https://stable-baselines3.readthedocs.io/en/master/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
